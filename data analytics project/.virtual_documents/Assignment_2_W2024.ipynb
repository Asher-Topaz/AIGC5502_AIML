





Use the link :"https://www.goodreads.com/quotes"
Get the Html file grabs and scrape all the details for each posting. 


import requests
from bs4 import BeautifulSoup
import pandas as pd
import json
from collections import OrderedDict


def scrape_quotes(url):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}
    response = requests.get(url, headers=headers)
    if response.status_code == 200:
        soup = BeautifulSoup(response.content, 'html.parser')
        quotes = soup.find_all('div', class_='quote')
        data = []
        for quote in quotes:
            quote_text = quote.find('div', class_='quoteText').text.strip().split('\n')[0]
            author = quote.find('span', class_='authorOrTitle').text.strip()
            tags = [tag.text.strip() for tag in quote.find_all('a', class_='smallText')]
            data.append({'Quote': quote_text, 'Author': author, 'Tags': tags})
        return data
    else:
        print("Failed to retrieve page:", url)
        return None



base_url = 'https://www.goodreads.com/quotes?page='


num_pages = 5


all_quotes = []



for page_num in range(1, num_pages+1):
    page_url = base_url + str(page_num)
    quotes_data = scrape_quotes(page_url)
    if quotes_data:
        all_quotes.extend(quotes_data)


quotes_df = pd.DataFrame(all_quotes)
quotes_df.head(10)












# install these libraries given below to run the program successfully, 
# if already installed ignore.
#pip install numpy
#pip install pandas
#pip seaborn
#pip matplotlib



import numpy as np 


import pandas as pd 


import matplotlib.pyplot as plt
import seaborn as sns








df= pd.read_csv('startup_funding.csv')
df.head()


df.dtypes


df['Date dd/mm/yyyy'] = pd.to_datetime(df['Date dd/mm/yyyy'], format='%d/%m/%Y')
df['Date dd/mm/yyyy'] = df['Date dd/mm/yyyy'].dt.year
funding_by_year = df.groupby('Year').size()















