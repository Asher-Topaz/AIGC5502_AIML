{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[1]:\n",
    "\n",
    "\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "\n",
    "from time import gmtime, strftime\n",
    "import os\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "smclient = boto3.Session().client(\"sagemaker\")\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "bucket = \"project-ml-pipeline\"\n",
    "prefix = \"sagemaker/DEMO-project-ml-pipeline\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[2]:\n",
    "\n",
    "\n",
    "role\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[3]:\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import io\n",
    "import time\n",
    "import json\n",
    "import sagemaker.amazon.common as smac\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[4]:\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[5]:\n",
    "\n",
    "\n",
    "test_data = pd.read_csv('credit_card_score.csv')\n",
    "test_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[6]:\n",
    "\n",
    "\n",
    "test_data.describe(include='all')   \n",
    "# Describe function gives us a discriptive data analysis table. This can be a good starting point as it can help view discrepencies in the columns. (df.describe() by default gives discriptive summary for only the numerical columns, we can use the df.describe(include='all') to view all comumns including the categorical ones.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[7]:\n",
    "\n",
    "\n",
    "test_data.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[8]:\n",
    "\n",
    "\n",
    "test_data.describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[9]:\n",
    "\n",
    "\n",
    "test_data.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[10]:\n",
    "\n",
    "\n",
    "test_data.columns.values#.columns.values gives us the names of the columns in our data in form of an array as below. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[11]:\n",
    "\n",
    "\n",
    "test_data.isnull().sum().sort_values(ascending=False).to_frame().T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[12]:\n",
    "\n",
    "\n",
    "cols= ['Age',\n",
    "       'Annual_Income', 'Monthly_Inhand_Salary', 'Interest_Rate', 'Num_of_Loan',\n",
    "       'Delay_from_due_date', 'Num_of_Delayed_Payment',\n",
    "       'Num_Credit_Inquiries', 'Credit_Mix',\n",
    "       'Outstanding_Debt', 'Credit_Utilization_Ratio',\n",
    "       'Credit_History_Age', 'Payment_of_Min_Amount',\n",
    "       'Total_EMI_per_month', 'Amount_invested_monthly',\n",
    "       'Payment_Behaviour', 'Monthly_Balance', 'Credit_Score']\n",
    "\n",
    "needed_data = test_data[cols]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[13]:\n",
    "\n",
    "\n",
    "# We will drop the monthly_Inhand_Salary because it is highly correlated with Annual income and it has a lot of missing values. We cn alternatively use the Anuual_Salary column to fill the missing values for monthly_salary and use the monthly_salary column for analysis. \n",
    "needed_data = needed_data.drop(['Monthly_Inhand_Salary'], axis = 1) \n",
    "needed_data.isnull().sum()  # We use .isnull().sum() to see how many of the columns have null values. Once we have an overview we can then figure our ways to get rid of the null values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[14]:\n",
    "\n",
    "\n",
    "# We will convert the column ['Monthly_Balance'] to string datatype to limit the number of characters in string because some of the the columns have enteries which exceed the limit of float datatype, which we will later cnvert it into for analysis. \n",
    "needed_data['Monthly_Balance'] = needed_data['Monthly_Balance'].astype(str)  \n",
    "# We use this column to restrict the characters to first 10 characters. This will cover all of the enteries for monthly balance as none of the monthly values exceed 10 digits.\n",
    "needed_data['Monthly_Balance'] = needed_data['Monthly_Balance'].str[:10]  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[15]:\n",
    "\n",
    "\n",
    "# First I will convert few columns from Object to Int or Float Data Type so that we can perform analysis on the data. \n",
    "\n",
    "\n",
    "needed_data['Annual_Income'] = needed_data['Annual_Income'].apply(lambda x: str(x).replace(\"_\",\" \"))  \n",
    "# Annual income column has a lot of unclean data with '_' at the end or starting of the string. We will replace them with space. \n",
    "needed_data['Num_of_Delayed_Payment'] = needed_data['Num_of_Delayed_Payment'].apply(lambda x: str(x).replace(\"None\",\"0\")) \n",
    "# Num_of_Delayed_Payment column has None, nan for null values. WE can replace them with '0'.\n",
    "needed_data['Num_of_Delayed_Payment'] = needed_data['Num_of_Delayed_Payment'].apply(lambda x: str(x).replace(\"_\",\" \"))    \n",
    "# Num_of_Delayed_Payment column has a lot of unclean data with '_' at the end or starting of the string. We will replace them with space. \n",
    "needed_data['Num_of_Delayed_Payment'] = needed_data['Num_of_Delayed_Payment'].apply(lambda x: str(x).replace(\"nan\",\"0\")) \n",
    "# Similarly we can clean data for other columns. \n",
    "needed_data['Num_of_Delayed_Payment'] = pd.to_numeric(needed_data['Num_of_Delayed_Payment'])\n",
    "needed_data['Outstanding_Debt'] = needed_data['Outstanding_Debt'].apply(lambda x: str(x).replace(\"_\",\" \")) \n",
    "needed_data['Outstanding_Debt'] = pd.to_numeric(needed_data['Outstanding_Debt'])                                           \n",
    "# We will use the to_numeric() function to convert the values from strng to integer.\n",
    "needed_data['Amount_invested_monthly'] = needed_data['Amount_invested_monthly'].apply(lambda x: str(x).replace(\"_\",\" \"))\n",
    "needed_data['Amount_invested_monthly'] = needed_data['Amount_invested_monthly'].apply(lambda x: str(x).replace(\"nan\",\"0\"))\n",
    "needed_data['Amount_invested_monthly'] = needed_data['Amount_invested_monthly'].apply(lambda x: str(x).replace(\"None\",\"0\"))\n",
    "needed_data['Amount_invested_monthly'] = pd.to_numeric(needed_data['Amount_invested_monthly'])\n",
    "needed_data['Monthly_Balance'] = needed_data['Monthly_Balance'].apply(lambda x: str(x).replace(\"_\",\" \"))\n",
    "needed_data['Monthly_Balance'] = needed_data['Monthly_Balance'].apply(lambda x: str(x).replace(\"None\",\"0\"))\n",
    "needed_data['Monthly_Balance'] = needed_data['Monthly_Balance'].apply(lambda x: str(x).replace(\"nan\",\"0\"))\n",
    "needed_data['Monthly_Balance'] = pd.to_numeric(needed_data['Monthly_Balance'])\n",
    "needed_data['Annual_Income'] = pd.to_numeric(needed_data['Annual_Income'])\n",
    "needed_data['Age'] = needed_data['Age'].apply(lambda x: str(x).replace(\"_\",\" \"))\n",
    "needed_data['Age'] = pd.to_numeric(needed_data['Age'])\n",
    "needed_data['Num_of_Loan'] = needed_data['Num_of_Loan'].apply(lambda x: str(x).replace(\"_\",\" \"))\n",
    "needed_data['Num_of_Loan'] = pd.to_numeric(needed_data['Num_of_Loan'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[16]:\n",
    "\n",
    "\n",
    "needed_data.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[17]:\n",
    "\n",
    "\n",
    "# We will view the data to figure out the further analysis. \n",
    "needed_data.describe(include = 'all')                                                 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[18]:\n",
    "\n",
    "\n",
    "# For the Credit_History_Age column we will need to find a way to convert the values into integer or float.\n",
    "# We can either remove the repeating parts of the string such as 'Years and Months' or we can grab the first two characters from left of the string for Year and the and 2 characters from 7th position right end. Or we can use both for easier way. \n",
    "# I will use the second option. \n",
    "\n",
    "# We will reate a copy of our data to perform the further analysis\n",
    "nd= needed_data.copy()\n",
    "nd['Credit_History_Age'] = nd['Credit_History_Age'].replace('nan', np.nan).fillna(0)   # We will fill the missing string values with 0 so we can convert to integer type later on.\n",
    "nd['Credit_History_Age'] = nd['Credit_History_Age'].astype(str)                        # To convert to string type. \n",
    "nd['History_Year'] = nd['Credit_History_Age'].str[ :2]                                 # To grab the first two characters of the string.                              \n",
    "nd['History_Month'] = nd['Credit_History_Age'].str[ -9:-7]                             # We will use the negative values to count the numbers from the right side of the data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[19]:\n",
    "\n",
    "\n",
    "nd['History_Year'] = pd.to_numeric(nd['History_Year'])                  # We use the pd.to_numeric to convert the column into integer type.\n",
    "nd['History_Month'] = pd.to_numeric(nd['History_Month']) \n",
    "nd['Credit_History_Age'] = nd['History_Year']*12 + nd['History_Month']  # Now that we have months and years we can make a combined columns with total months for Credit_History_Age.\n",
    "nd = nd.drop(['History_Year'], axis = 1)                                # Now that we have a consolidated column for Credit_History_Age we can drop the History_Year and History_month columns.\n",
    "nd = nd.drop(['History_Month'], axis = 1)\n",
    "nd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[20]:\n",
    "\n",
    "\n",
    "# Now we have 4 columns left as objects that have categorical or ordinal data. \n",
    "{column: list(nd[column].unique()) for column in nd.select_dtypes('object').columns}  # We use this code to view diffferent inputs in each of the coategorical column. \n",
    "nd.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[21]:\n",
    "\n",
    "\n",
    "# First we will identify which columns are nominal and which ones are ordinal.\n",
    "# Credit_Mix and Credit_score are ordinal.So we can provide order to them manually. \n",
    "# Payment_of_Min_Amount, Payment_Behaviour can be considered as catecorical and we can use onehotencoder to provide dummy variables.\n",
    "\n",
    "nd = nd[nd.Payment_Behaviour != '!@9#%8']                                  # After seeing the different values in the columns we find out that some cells of column '[Payment_Behaviour]' have '!@9#%8' as a conditional values  We will delete all the observations with _ as value. \n",
    "nd = nd[nd.Payment_of_Min_Amount != 'NM']                                  # Similarly some of the cells in ['Payment_of_Min_Amount'] column are listed as 'NM'. \n",
    "\n",
    "# We will now define functions for one hot encoding and ordinal encoding. So I will create 2 functions which we can call later for ordinal or categorical data. We will use onehot encoding for categorical data and ordinal encoding for Ordinal. \n",
    "\n",
    "def ordinal_encode(df, column, ordering):\n",
    "    df = df.copy()\n",
    "    df[column] = df[column].apply(lambda x: ordering.index(x))\n",
    "    return df\n",
    "\n",
    "def onehot_encode(df, column, prefix):\n",
    "    df = df.copy()\n",
    "    dummies = pd.get_dummies(df[column],dtype=int, prefix=prefix)\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "    df = df.drop(column, axis=1)\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[22]:\n",
    "\n",
    "\n",
    "# We will create new array to define the order of the ordinal variables so that we can use the order in the ordinal encoder() function. We will do this for all three ordinal columns i.e.: 'Credit_Mix', 'Credit_Score', 'Payment_Behaviour'.\n",
    "Credit_Mix_order = [ '_',\n",
    "                    'Bad',\n",
    "                    'Standard',\n",
    "                    'Good']\n",
    "\n",
    "Payment_Behaviour_order = ['Low_spent_Small_value_payments',\n",
    "                          'Low_spent_Medium_value_payments',\n",
    "                          'High_spent_Small_value_payments',\n",
    "                          'Low_spent_Large_value_payments',\n",
    "                           'High_spent_Medium_value_payments',\n",
    "                           'High_spent_Large_value_payments']\n",
    "\n",
    "Credit_Score_order = ['Poor',\n",
    "                      'Standard',\n",
    "                      'Good' ]\n",
    "\n",
    "\n",
    "nd = ordinal_encode(nd, 'Credit_Mix', ordering = Credit_Mix_order)                       # We will call the ordical_encode() function for encoding different columns. Three inputs are the name of dataset, name of column and the order array. \n",
    "nd= ordinal_encode(nd, 'Payment_Behaviour', ordering = Payment_Behaviour_order )\n",
    "nd= ordinal_encode(nd, 'Credit_Score', ordering = Credit_Score_order )\n",
    "\n",
    "\n",
    "nd = onehot_encode(nd, 'Payment_of_Min_Amount', prefix = 'PMA' )                         # Payment_of_Min_Amount is a categorical variable so we use onehot encoding for this.\n",
    "                                  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[23]:\n",
    "\n",
    "\n",
    "cleaned_data = nd    # We will now visualize the cleaned table and name it as cleaned_data. \n",
    "cleaned_data.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[24]:\n",
    "\n",
    "\n",
    "from matplotlib import colors\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "sns.set(rc={\"axes.facecolor\":\"#CEE8C8\",\"figure.facecolor\":\"#FFFFF5\"})\n",
    "pallet = [\"#FAD89F\", \"#53726D\", \"#424141\", \"#FFFFF5\"]\n",
    "cmap = colors.ListedColormap([\"#ACC7EF\", \"#FAD89F\", \"#53726D\", \"#424141\", \"#FFFFF5\"])\n",
    "#Plotting following features\n",
    "To_Plot = [ \"Age\", \"Interest_Rate\", \"Delay_from_due_date\", \"Num_of_Delayed_Payment\", \"Credit_Mix\"]\n",
    "print(\"Reletive Plot Of Features\")\n",
    "plt.figure(figsize=(20,20))  \n",
    "sns.pairplot(cleaned_data[To_Plot], hue= \"Credit_Mix\", palette= ([\"#ACC7EF\", \"#FAD89F\", \"#53726D\", \"#424141\"]))\n",
    "#Taking hue \n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[25]:\n",
    "\n",
    "\n",
    "# We can clearly see that Age has few outliiers so we will limit it to 100. \n",
    "\n",
    "q = cleaned_data['Interest_Rate'].quantile(0.95)\n",
    "cleaned_data = cleaned_data[cleaned_data['Interest_Rate']<q]\n",
    "\n",
    "cleaned_data['Num_of_Loan'] = cleaned_data['Num_of_Loan'].apply(lambda x: str(x).replace(\"-\",\"0\"))\n",
    "cleaned_data['Num_of_Loan'] = cleaned_data['Num_of_Loan'].str[ :1] \n",
    "cleaned_data['Num_of_Loan'] = pd.to_numeric(cleaned_data['Num_of_Loan'])  \n",
    "\n",
    "cleaned_data['Age'] = cleaned_data['Age'].apply(lambda x: str(x).replace(\"-\",\" \"))\n",
    "cleaned_data['Age'] = cleaned_data['Age'].str[ :3]   \n",
    "cleaned_data['Age'] = pd.to_numeric(cleaned_data['Age'])  \n",
    "cleaned_data = cleaned_data[(cleaned_data[\"Age\"]> 18)]\n",
    "cleaned_data = cleaned_data[(cleaned_data[\"Age\"]< 100)]\n",
    "\n",
    "cleaned_data['Delay_from_due_date'] = cleaned_data['Delay_from_due_date'].apply(lambda x: str(x).replace(\"-\",\" \"))\n",
    "cleaned_data['Delay_from_due_date'] = pd.to_numeric(cleaned_data['Delay_from_due_date'])  \n",
    "\n",
    "cleaned_data['Num_of_Delayed_Payment'] = cleaned_data['Num_of_Delayed_Payment'].apply(lambda x: str(x).replace(\"-\",\" \"))\n",
    "cleaned_data['Num_of_Delayed_Payment'] = pd.to_numeric(cleaned_data['Num_of_Delayed_Payment'])  \n",
    "\n",
    "q = cleaned_data['Num_of_Delayed_Payment'].quantile(0.95)\n",
    "cleaned_data = cleaned_data[cleaned_data['Num_of_Delayed_Payment']<q]\n",
    "cleaned_data = cleaned_data.fillna('0')\n",
    "\n",
    "cleaned_data.describe()\n",
    "cleaned_data.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[26]:\n",
    "\n",
    "\n",
    "sns.set(rc={\"axes.facecolor\":\"#CEE8C8\",\"figure.facecolor\":\"#FFFFF5\"})\n",
    "pallet = [\"#FAD89F\", \"#53726D\", \"#424141\", \"#FFFFF5\"]\n",
    "cmap = colors.ListedColormap([\"#ACC7EF\", \"#FAD89F\", \"#53726D\", \"#424141\", \"#FFFFF5\"])\n",
    "#Plotting following features\n",
    "To_Plot = [ \"Age\", \"Interest_Rate\", \"Delay_from_due_date\", \"Num_of_Delayed_Payment\", \"Credit_Mix\"]\n",
    "print(\"Reletive Plot Of Features\")\n",
    "plt.figure(figsize=(20,20))  \n",
    "sns.pairplot(cleaned_data[To_Plot], hue= \"Credit_Mix\", palette= ([\"#ACC7EF\", \"#FAD89F\", \"#53726D\", \"#424141\"]))\n",
    "#Taking hue \n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[27]:\n",
    "\n",
    "\n",
    "#sns.set(rc={\"axes.facecolor\":\"#CEE8C8\",\"figure.facecolor\":\"#FFFFF5\"})\n",
    "#pallet = [\"#FAD89F\", \"#CEE8C8\", \"#53726D\", \"#FFFFF5\"]\n",
    "#cmap = colors.ListedColormap([\"#ACC7EF\", \"#FAD89F\", \"#CEE8C8\", \"#53726D\", \"#FFFFF5\"])\n",
    "corrmat= cleaned_data.corr()\n",
    "plt.figure(figsize=(20,20))  \n",
    "sns.heatmap(corrmat,annot=True,  center=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[28]:\n",
    "\n",
    "\n",
    "cleaned_data.sample(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[29]:\n",
    "\n",
    "\n",
    "target_variable = \"Credit_Score\"\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = cleaned_data.drop(target_variable, axis=1)\n",
    "y = cleaned_data[target_variable]\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=1729)\n",
    "X_validation, X_test, y_validation, y_test = train_test_split(X_temp, y_temp, test_size=0.33, random_state=1729)\n",
    "\n",
    "# Save the splits into CSV files\n",
    "train_data = pd.concat([y_train, X_train], axis=1)\n",
    "validation_data = pd.concat([y_validation, X_validation], axis=1)\n",
    "test_data = pd.concat([y_test, X_test], axis=1)\n",
    "\n",
    "train_data.to_csv(\"train.csv\", index=False, header=False)\n",
    "validation_data.to_csv(\"validation.csv\", index=False, header=False)\n",
    "test_data.to_csv(\"test.csv\", index=False, header=False)\n",
    "\n",
    "pd.read_csv(\"train.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[30]:\n",
    "\n",
    "\n",
    "boto3.Session().resource(\"s3\").Bucket(bucket).Object(\n",
    "    os.path.join(prefix, \"train/train.csv\")\n",
    ").upload_file(\"train.csv\")\n",
    "boto3.Session().resource(\"s3\").Bucket(bucket).Object(\n",
    "    os.path.join(prefix, \"validation/validation.csv\")\n",
    ").upload_file(\"validation.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[68]:\n",
    "\n",
    "\n",
    "from sagemaker.image_uris import retrieve\n",
    "\n",
    "training_image = retrieve(framework=\"xgboost\", region=region, version=\"1.7-1\")\n",
    "\n",
    "s3_input_train = \"s3://{}/{}/train\".format(bucket, prefix)\n",
    "s3_input_validation = \"s3://{}/{}/validation/\".format(bucket, prefix)\n",
    "\n",
    "training_job_definition = {\n",
    "    \"AlgorithmSpecification\": {\"TrainingImage\": training_image, \"TrainingInputMode\": \"File\"},\n",
    "    \"InputDataConfig\": [\n",
    "        {\n",
    "            \"ChannelName\": \"train\",\n",
    "            \"CompressionType\": \"None\",\n",
    "            \"ContentType\": \"csv\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": s3_input_train,\n",
    "                }\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            \"ChannelName\": \"validation\",\n",
    "            \"CompressionType\": \"None\",\n",
    "            \"ContentType\": \"csv\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\",\n",
    "                    \"S3DataType\": \"S3Prefix\",\n",
    "                    \"S3Uri\": s3_input_validation,\n",
    "                }\n",
    "            },\n",
    "        },\n",
    "    ],\n",
    "    \"OutputDataConfig\": {\"S3OutputPath\": \"s3://{}/{}/output\".format(bucket, prefix)},\n",
    "    \"ResourceConfig\": {\"InstanceCount\": 1, \"InstanceType\": \"ml.m4.xlarge\", \"VolumeSizeInGB\": 10},\n",
    "    \"RoleArn\": role,\n",
    "    \"StaticHyperParameters\": {\n",
    "    \n",
    "        \n",
    "        \"verbosity\": \"0\",\n",
    "        \"objective\": \"multi:softmax\",\n",
    "        \"num_class\": \"10\",\n",
    "        \n",
    "        \n",
    "        \n",
    "    },\n",
    "    \"StoppingCondition\": {\"MaxRuntimeInSeconds\": 43200},\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[69]:\n",
    "\n",
    "\n",
    "from time import gmtime, strftime, sleep\n",
    "\n",
    "tuning_job_name = \"xgboost-project11-\" + strftime(\"%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "print(tuning_job_name)\n",
    "\n",
    "tuning_job_config = {\n",
    "    \"ParameterRanges\": {\n",
    "        \"CategoricalParameterRanges\": [],\n",
    "        \"ContinuousParameterRanges\": [\n",
    "            {\n",
    "                \"MaxValue\": \"0.5\",\n",
    "                \"MinValue\": \"0.1\",\n",
    "                \"Name\": \"eta\",\n",
    "            },\n",
    "            {\n",
    "                \"MaxValue\": \"5\",\n",
    "                \"MinValue\": \"0\",\n",
    "                \"Name\": \"gamma\",\n",
    "            },\n",
    "            {\n",
    "                \"MaxValue\": \"120\",\n",
    "                \"MinValue\": \"0\",\n",
    "                \"Name\": \"min_child_weight\",\n",
    "            },\n",
    "            {\n",
    "                \"MaxValue\": \"1\",\n",
    "                \"MinValue\": \"0.5\",\n",
    "                \"Name\": \"subsample\",\n",
    "            },\n",
    "            {\n",
    "                \"MaxValue\": \"2\",\n",
    "                \"MinValue\": \"0\",\n",
    "                \"Name\": \"alpha\",\n",
    "            },\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        ],\n",
    "        \"IntegerParameterRanges\": [\n",
    "            {\n",
    "                \"MaxValue\": \"10\",\n",
    "                \"MinValue\": \"0\",\n",
    "                \"Name\": \"max_depth\",\n",
    "            },\n",
    "            {\n",
    "                \"MaxValue\": \"50\",\n",
    "                \"MinValue\": \"1\",\n",
    "                \"Name\": \"num_round\",\n",
    "            },\n",
    "            \n",
    "           \n",
    "        ],\n",
    "    },\n",
    "    \"ResourceLimits\": {\"MaxNumberOfTrainingJobs\": 10, \"MaxParallelTrainingJobs\": 3},\n",
    "    \"Strategy\": \"Bayesian\",\n",
    "    \"HyperParameterTuningJobObjective\": {\"MetricName\": \"validation:merror\", \"Type\": \"Minimize\"},\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[70]:\n",
    "\n",
    "\n",
    "smclient.create_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=tuning_job_name,\n",
    "    HyperParameterTuningJobConfig=tuning_job_config,\n",
    "    TrainingJobDefinition=training_job_definition,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[71]:\n",
    "\n",
    "\n",
    "smclient.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName=tuning_job_name)[\n",
    "    \"HyperParameterTuningJobStatus\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
